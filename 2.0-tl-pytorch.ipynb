{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please Note**: We updated the requirements.txt\n",
    "\n",
    "Please install the new requirements before editing this exercise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from vll.utils.download import download_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage\n",
    "import skimage.io\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from models.mnist.simple_cnn import Net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "(2 points)\n",
    "\n",
    "In this task, you will learn some basic tensor operations using the PyTorch library.\n",
    "\n",
    "Reference for torch: https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19.])\n",
      "tensor([0.0000e+00, 8.6695e-03, 1.6797e+00, 7.0190e-01, 1.0022e+00, 2.0373e+00,\n",
      "        9.4046e-01, 6.8065e+00, 4.3440e+00, 4.8502e+00, 4.3251e+00, 7.2556e-01,\n",
      "        9.8594e+00, 7.2642e+00, 1.3380e+01, 9.7810e+00, 9.1109e+00, 4.1246e+00,\n",
      "        1.1715e+01, 1.8246e+01])\n",
      "tensor([-1.0000e+00, -9.9133e-01,  6.7970e-01, -2.9810e-01,  2.2395e-03,\n",
      "         1.0373e+00, -5.9537e-02,  5.8065e+00,  3.3440e+00,  3.8502e+00,\n",
      "         3.3251e+00, -2.7444e-01,  8.8594e+00,  6.2642e+00,  1.2380e+01,\n",
      "         8.7810e+00,  8.1109e+00,  3.1246e+00,  1.0715e+01,  1.7246e+01])\n",
      "tensor(1.0373)\n",
      "tensor([-0., -0., 0., -0., 0., 0., -0., 0., 0., 0., 0., -0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Create a numpy array that looks like this: [0, 1, 2, ..., 19]\n",
    "arr = np.arange(20)\n",
    "print(arr)\n",
    "\n",
    "# Convert the numpy array to a torch tensor\n",
    "tensor = torch.tensor(arr, dtype=torch.float32)  # Specify dtype as float to not face compatibility issues later\n",
    "print(tensor)\n",
    "\n",
    "# Create a tensor that contains random numbers.\n",
    "# It should have the same size like the numpy array.\n",
    "# Multiply it with the previous tensor.\n",
    "rand_tensor = torch.rand_like(tensor)\n",
    "tensor = tensor * rand_tensor\n",
    "print(tensor)\n",
    "\n",
    "# Create a tensor that contains only 1s.\n",
    "# It should have the same size like the numpy array.\n",
    "# Substract it from the previous tensor.\n",
    "tensor = tensor - torch.ones_like(tensor)\n",
    "print(tensor)\n",
    "\n",
    "# Get the 5th element using a index.\n",
    "element = tensor[5]\n",
    "print(element)\n",
    "\n",
    "# Create a tensor that contains only 0s.\n",
    "# It should have the same size like the numpy array.\n",
    "# Multiply it with the previous tensor without any assignment (in place).\n",
    "tensor *= torch.zeros_like(tensor)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512, 3])\n",
      "786432\n",
      "torch.Size([1, 786432])\n",
      "torch.Size([786432])\n",
      "torch.Size([512, 512, 3])\n",
      "tensor(91404744.)\n",
      "tensor(116.2271)\n",
      "tensor(255.)\n"
     ]
    }
   ],
   "source": [
    "# Load the image from the last exercise as RGB image.\n",
    "image = skimage.io.imread(\"data/pepo.jpg\")\n",
    "\n",
    "# Convert the image to a tensor\n",
    "image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "# Print its shape\n",
    "print(image.shape) # torch.Size([512, 512, 3])\n",
    "# Flatten the image\n",
    "image = image.view(-1)\n",
    "print(len(image))\n",
    "\n",
    "# Add another dimension resulting in a 1x78643 tensor\n",
    "image = image.unsqueeze(0)\n",
    "print(image.shape)\n",
    "\n",
    "# Revert the last action\n",
    "image = image.squeeze(0)\n",
    "print(image.shape)\n",
    "\n",
    "# Reshape the tensor, so that it has the original 2D dimensions\n",
    "image = image.view(512, 512, 3)\n",
    "print(image.shape)\n",
    "\n",
    "# Calculate the sum, mean and max of the tensor\n",
    "print(torch.sum(image))\n",
    "print(torch.mean(image))\n",
    "print(torch.max(image))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "(2 points)\n",
    "\n",
    "Use Autograd to perform operations on a tensor and output then gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6842, 0.9334],\n",
      "        [0.1779, 0.3194]], requires_grad=True)\n",
      "tensor([[2.6842, 2.9334],\n",
      "        [2.1779, 2.3194]], grad_fn=<AddBackward0>)\n",
      "tensor([[7.2047, 8.6047],\n",
      "        [4.7431, 5.3798]], grad_fn=<PowBackward0>)\n",
      "tensor(6.4831, grad_fn=<MeanBackward0>)\n",
      "tensor([[1.3421, 1.4667],\n",
      "        [1.0889, 1.1597]])\n",
      "False\n",
      "tensor(6.4831)\n"
     ]
    }
   ],
   "source": [
    "# Create a random 2x2 tensor which requires gradients\n",
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# Create another tensor by adding 2.0\n",
    "y = x + 2.0\n",
    "print(y)\n",
    "\n",
    "# Create a third tensor z = y^2\n",
    "z = y ** 2\n",
    "print(z)\n",
    "\n",
    "# Compute out as the mean of values in z\n",
    "out = torch.mean(z)\n",
    "print(out)\n",
    "\n",
    "# Perform back propagation on out\n",
    "out.backward()\n",
    "\n",
    "# Print the gradients dout/dx\n",
    "print(x.grad)\n",
    "\n",
    "# Create a copy of y whithout gradients\n",
    "y2 = y.detach()\n",
    "print(y2.requires_grad)\n",
    "\n",
    "# Perform the mean operation on z\n",
    "# with gradients globally disabled\n",
    "with torch.no_grad():\n",
    "    out = torch.mean(z)\n",
    "    print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "(3 points)\n",
    "\n",
    "Implement a Dataset class for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mnist.tar.gz\n",
      "Extract mnist.tar.gz\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# We first download the MNIST dataset\n",
    "download_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    \"\"\"\n",
    "    Dataset class for MNIST\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\"\n",
    "        root -- path to either \"training\" or \"testing\"\n",
    "        \n",
    "        transform -- transform (from torchvision.transforms)\n",
    "                     to be applied to the data\n",
    "        \"\"\"\n",
    "        # save transforms\n",
    "        self.transform = transform\n",
    "        \n",
    "        # TODO: create a list of all subdirectories (named like the classes) \n",
    "        #       within the dataset root\n",
    "        self.class_names = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
    "        \n",
    "        # TODO: create a list of paths to all images\n",
    "        #       with the ground truth label\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(root, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(class_idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the lenght of the dataset (number of images)\n",
    "        \"\"\"\n",
    "        # TODO: return the length (number of images) of the dataset\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Loads and returns one image as floating point numpy array\n",
    "        \n",
    "        index -- image index in [0, self.__len__() - 1]\n",
    "        \"\"\"\n",
    "        # TODO: load the ith image as an numpy array (dtype=float32)\n",
    "        img_path = self.image_paths[index]\n",
    "        img = skimage.io.imread(img_path).astype(np.float32)\n",
    "        \n",
    "        \n",
    "        # TODO: apply transforms to the image (if there are any)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # TODO: return a tuple (transformed image, ground truth)\n",
    "        return img, self.labels[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "(3 points)\n",
    "\n",
    "You can now load a pretrained neural network model we provide.\n",
    "Your last task is to run the model on the MNIST test dataset, plot some example images with the predicted labels and compute the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1347: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5717.0068, -7425.4971, -7914.2905, -4664.5850, -7091.9590,     0.0000,\n",
      "         -4960.5947, -7397.2656, -4164.8223, -4856.1074]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 1, 28, 28) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3j/gcq4k6y56x19103qcjqq2v040000gn/T/ipykernel_7019/1595558014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/3j/gcq4k6y56x19103qcjqq2v040000gn/T/ipykernel_7019/1595558014.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# TODO: disable axis and show image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# TODO: show the predicted class next to each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5607\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5609\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5610\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5611\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    707\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    708\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 709\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    710\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 1, 28, 28) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def validate(model, data_loader):\n",
    "    # TODO: Create a 10x10 grid of subplots\n",
    "    fig, axs = plt.subplots(10, 10, figsize=(1, 1))\n",
    "    axes = axs.flatten()\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0 # count for correct predictions\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, item in enumerate(data_loader):\n",
    "            # TODO: unpack item into image and ground truth\n",
    "            #       and run network on them\n",
    "            image, ground_trouth = item\n",
    "            output = model(image)\n",
    "            print(output)\n",
    "            # TODO: get class with highest probability\n",
    "            predicted_class = output.argmax(dim=1)\n",
    "            \n",
    "            # TODO: check if prediction is correct\n",
    "            #       and add it to correct count\n",
    "            if predicted_class == ground_trouth:\n",
    "                correct += 1\n",
    "                cmap = 'Reds'\n",
    "            elif predicted_class != ground_trouth:\n",
    "                cmap = 'gray'\n",
    "            # plot the first 100 images\n",
    "            if i < 100:\n",
    "                # TODO: compute position of ith image in the grid\n",
    "                ax = axes[i]\n",
    "\n",
    "                # TODO: convert image tensor to numpy array\n",
    "                #       and normalize to [0, 1]\n",
    "                img_np = np.array(image)\n",
    "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())                # TODO: make wrongly predicted images red\n",
    "                \n",
    "                # TODO: disable axis and show image\n",
    "                ax.axis('off')\n",
    "                ax.imshow(img_np, cmap=cmap)\n",
    "                \n",
    "                # TODO: show the predicted class next to each image\n",
    "                ax.settitle(f'Predicted label: {predicted_class}')\n",
    "            elif i == 100:\n",
    "                plt.show()\n",
    "                break\n",
    "                \n",
    "    \n",
    "    # TODO: compute and print the prediction accuracy in percent\n",
    "\n",
    "# create a DataLoader using the implemented MNIST dataset class\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    MNIST('data/mnist/testing',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=1, shuffle=True)\n",
    "\n",
    "# create the neural network\n",
    "model = Net()\n",
    "\n",
    "# load the statedict from 'models/mnist/simple_cnn.pt'\n",
    "model.load_state_dict(torch.load('models/mnist/simple_cnn.pt'))\n",
    "\n",
    "# validate the model\n",
    "validate(model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dcv-students",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
